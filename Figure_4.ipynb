{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Figures 4 of the paper on _Quantitative storytelling in the making of a composite indicator_ -->\n",
    "# Figures 4 of the paper on _Quantitative storytelling in the making of a composite indicator_\n",
    "<!-- dom:AUTHOR: Samuele Lo Piano at Universitat Oberta de Catalunya, Barcelona -->\n",
    "<!--\n",
    "Author: -->  \n",
    "**Samuele Lo Piano**, Universitat Oberta de Catalunya, Barcelona  \n",
    "Date: **Aug 03, 2019**\n",
    "\n",
    "This notebook describes how Figure 4 in the paper _Quantitative storytelling in the making of a composite indicator_ has been \n",
    "produced. The layout has been inspired by this [rich collection](https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/?utm_campaign=shareaholic&utm_medium=twitter&utm_source=socialnetwork) of notebooks, which you are warmly invited to browse. The preparation of the _notebook_ as regards Paul Romer's quantitative story telling has been directly retrieved from\n",
    "his [_GitHub_ repository](https://github.com/paulromer149/DB-Calcs).\n",
    "\n",
    "## Step 1 - import all the relevant libraries \n",
    "\n",
    "This step is essential to develop your code flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library for managing data \n",
    "\n",
    "import matplotlib.colors as colors\n",
    "colors_list = list(colors._colors_full_map.values())\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following passages include the details of Romer's computations to get consistent metrics over the years as to allow to fairly evaluate Chile's world rank of the _Ease of doing business score_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the notebook so that it can display all countries in a dataframe\n",
    "#\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for reading in the data worksheet from the World Bank's Doing Business website\n",
    "#\n",
    "data_url = 'http://www.doingbusiness.org/~/media/WBG/DoingBusiness/Documents/Data/DB18-Historical-data-complete-data-with-DTFs.xlsx'\n",
    "sheet = \"All Data\"\n",
    "header_row = 1 # the convention in Python is 0-based indexes; in the worksheet the header is in row 2 \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data sheet into a pandas dataframe df\n",
    "# Use the header to get variable / column names \n",
    "# Convert the value 'No Practice' to NaN - not a number \n",
    "# \n",
    "df = pd.read_excel(data_url,sheet_name = sheet, header = header_row, na_values = ['No Practice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the #-delimiter on the line below, to see rows that include Bangladesh\n",
    "#\n",
    "#df[150:200]\n",
    "#\n",
    "# It is one of the countries where a second city was added. It is because of these additions \n",
    "#    that later there is a cell that drops rows if \n",
    "# \n",
    "#       \"len(df2.loc[i,'code']) != 3\"\n",
    "#  \n",
    "#    that is, if the code has more or less characters than the 3-letters that were used for the original observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with keys that are the existing variable names and values that are the new names I use\n",
    "\n",
    "# Note that the import from the excel sheet includes some random line break characters '\\n'\n",
    "\n",
    "rename_variables = {'Country code': 'code', \n",
    "                    'DB Year': 'year', \n",
    "                    'Procedures - Men (number) ': 's_procs',\n",
    "                    'Time - Men (days)': 's_time', \n",
    "                    'Cost - Men (% of income per capita)': 's_cost', \n",
    "                    'Minimum capital (% of income per capita)': 's_min_cap', \n",
    "                    'Procedures (number)': 'cn_procs', 'Time (days)': 'cn_time', \n",
    "                    'Cost (% of Warehouse value)': 'cn_cost', \n",
    "                    'Procedures (number).1': 'e_procs', \n",
    "                    'Time (days).1': 'e_time', \n",
    "                    'Cost (% of income per capita)': 'e_cost', \n",
    "                    'Procedures (number).2': 'rp_procs', \n",
    "                    'Time (days).2': 'rp_time', \n",
    "                    'Cost (% of property value)': 'rp_cost', \n",
    "                    'Strength of legal rights index (0-12) (DB15-18 methodology) ': 'ct_s', \n",
    "                    'Depth of credit information index (0-8) (DB15-18 methodology) ': 'ct_d', \n",
    "                    'Extent of conflict of interest regulation index (0-10)\\n(DB15-18 methodology) ':'pm_cft', \n",
    "                    'Extent of shareholder governance index (0-10) (DB15-18 methodology) ':'pm_gv', \n",
    "                    'Payments (number per year)': 't_p', 'Time (hours per year)': \n",
    "                    't_t', 'Total tax rate (% of profit)': 't_tr', \n",
    "                    'Time (days).3': 'en_time', \n",
    "                    'Cost (% of claim)': 'en_cost', \n",
    "                    'Recovery rate (cents on the dollar)': 'ri_r', \n",
    "                    'Strength of insolvency framework index (0-16) (DB15-18 methodology)': 'ri_s'\n",
    "                   }\n",
    "\n",
    "# Treat the variable names as a set so that set subtraction specifies the one to drop\n",
    "#\n",
    "all_vars = set(df.columns.values)\n",
    "\n",
    "# Old_names from for the variables that the code keeps and renames \n",
    "old_names = set(rename_variables.keys())\n",
    "\n",
    "# The implied set of variables to drop\n",
    "vars_to_drop = all_vars - old_names\n",
    "\n",
    "# Create a list with the new names for the variables that remain \n",
    "new_names = list(rename_variables.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an independent copy of the dataframe.\n",
    "#\n",
    "# If I want to redo my calculations as I work interactively, I can just re-execute this cell without \n",
    "#     reloading the data from the external website\n",
    "# \n",
    "df2 = pd.DataFrame(df.copy(deep = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the set of variables that I will drop from a set to a list \n",
    "#\n",
    "el_2 = list(vars_to_drop)\n",
    "#\n",
    "#len(df2.columns.values) # count of variables before the drop \n",
    "#\n",
    "# Drop the variables \n",
    "#\n",
    "df2.drop(el_2, axis = 1, inplace = True)\n",
    "#\n",
    "#len(df2.columns.values) # to check the variable count after the drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns / variables that remain  \n",
    "#\n",
    "df2.rename(columns=rename_variables, inplace = True)\n",
    "#\n",
    "# To verify that rename and drop affects df2 but not df, uncomment one, then other of next two lines and compare \n",
    "#df\n",
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop years before and including 2013 \n",
    "#\n",
    "df2.drop(labels = [i for i in df2.index if df2.loc[i,'year'] <= 2013], inplace = True)\n",
    "#\n",
    "#len(df2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As noted above, drop recently added extra cities  \n",
    "# \n",
    "df2.drop(labels = [i for i in df2.index if len(df2.loc[i,'code']) != 3], inplace = True)\n",
    "#\n",
    "#len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a multi-index for the remaining variables \n",
    "\n",
    "df2.set_index(['year', 'code'], inplace=True)\n",
    "\n",
    "#df2 # inspect results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dataframe to store the normalized or distance to the frontier (DTF) values \n",
    "#    for different indicators. \n",
    "#\n",
    "dtf = pd.DataFrame(df2.copy(deep = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the indicators that remain \n",
    "\n",
    "el_3 = list(dtf.columns.values)\n",
    "#el_3\n",
    "#len(el_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicators in high are ones where bigger values are better; opposite for variables in low \n",
    "#\n",
    "high = ['ct_s', 'ct_d', 'pm_cft', 'pm_gv',  'ri_r', 'ri_s' ]\n",
    "low = ['s_procs', 's_time', 's_cost', 's_min_cap', 'cn_procs', 'cn_time',\n",
    "       'cn_cost', 'e_procs', 'e_time', 'e_cost', 'rp_procs', 'rp_time',  \n",
    "       'rp_cost', 't_p', 't_t', 't_tr', 'en_time', 'en_cost']\n",
    "\n",
    "# len(high) + len(low) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop calculates the distance to the frontier for the 24 variables that are available in a consistent  \n",
    "#    form for the years I consider, DB2014-18, or calendar 2013-17. \n",
    "\n",
    "# These normalized values are stored in the dtf dataframe. The raw values remainin in the df2 dataframe. \n",
    "\n",
    "# The loop defines the distance to the frontier by taking the biggest and smallest values for \n",
    "#    each variable in any year from DB years 2014-18. \n",
    "\n",
    "# I wrote the code as I did assuming that I would use the max and min in each year; then I found \n",
    "#    that they change over time, sometimes substantially. This is the type of issue that I understand only \n",
    "#    if I work directly with the data myself. \n",
    "\n",
    "# This problem is that the min (worst) value for an indicator can change dramatically based on \n",
    "#    what happens in a single country with a very bad business environment. \n",
    "#    So I added the two lines that calculate mn_m and mx_m by taking the min and\n",
    "#    max over all DB years from 2014 to 2018. This decision influences the relative influence that \n",
    "#    different indicators have in my results.  \n",
    "\n",
    "# This is an important point. Suppose that it takes every other country between 10 and 100 days to \n",
    "#    to issue a permit, but in one laggard it takes 10,000 days. Then all other countries will have a DTF \n",
    "#    score for this indicator in the range 10/10,000 to 100/10,000. In this case, a country that takes only \n",
    "#    10 days gets almost no recognition for its better performance relative to a country that takes 100. \n",
    "\n",
    "# The DTF value for this permit indicator will be 0.999 for the country that takes 10 days \n",
    "#    and 0.990 for the country that takes 100 days. When this indicator is averaged along with 8 or 9 others, \n",
    "#    it will have an effect on the overall indicator that is visible only in the third decimal place. It will \n",
    "#    be swamped by variation in other indicators. \n",
    "\n",
    "# My choice is not one that I would defend as being the right way to determine the relative influence of \n",
    "#    different indicators. It underweights indicators with a fat lower (that is worse) tail. I haven't explored \n",
    "#    the sensitivity of the results for Chile to alternative choices because I didn't want to be accused of \n",
    "#    manipulating the data to get some particular outcome. \n",
    "\n",
    "# For my purpose, the choice I made had the advantage that it is arbitrary and leads to rankings for \n",
    "#    countries that do not change from year to year because of year to year **changes** in the min (worst) \n",
    "#    value of an indicator in some lagging country. My choice ensures that he range from best to worst, \n",
    "#    and hence the relative influence of each indicator, stays fixed over all the years that I consider. \n",
    "\n",
    "# One of the advantages of making this code available is that it lets others do their own sensitivity analysis\n",
    "#    with respect to this or any other issue. \n",
    "\n",
    "# The approach used by the Doing Business team addresses this concern in a different way. \n",
    "#    For most indicators (but not all), they also take the min and max over a five year interval.\n",
    "#    See the description of their approach here: \n",
    "\n",
    "#     http://www.doingbusiness.org/~/media/WBG/DoingBusiness/Documents/Annual-Reports/English/DB18-Chapters/DB18-DTF-and-DBRankings.pdf\n",
    "\n",
    "# To view the values for the min and max for each variable over all years or year by year, uncomment the \n",
    "#    print statements in this loop.  \n",
    "\n",
    "for i in range(len(el_3)):\n",
    "    mn = df2.groupby(['year'])[el_3[i]].min()\n",
    "    #print ('mn = ', mn)\n",
    "    mn_m = mn.min()\n",
    "    #print('mn_m ',mn_m)\n",
    "    mx = df2.groupby(['year'])[el_3[i]].max()\n",
    "    #print ('mx = ', mx)\n",
    "    mx_m = mx.max()\n",
    "    #print('mx_m ',mx_m)\n",
    "    if el_3[i] in low:\n",
    "        dtf[el_3[i]] = (mx_m - df2[el_3[i]]) / (mx_m - mn_m)\n",
    "    else:\n",
    "        dtf[el_3[i]] = (df2[el_3[i]] - mn_m)  / (mx_m - mn_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2[4:5]  # Test raw data for visual comparison with Bank numbers from spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtf[4:5]  # Test dtf or normalized data for comparison with Bank numbers on Afghanistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the Bank's hierarchical procedure; average the sub-components of the different indicators\n",
    "\n",
    "# d_ => prefix that means \"distance to ...\"\n",
    "# s => Starting A Business ...\n",
    "# cn => Construction Permits \n",
    "# e => Getting Electricity \n",
    "# rp => Registering Property \n",
    "# ct => Contract Enforcement \n",
    "# pm => Protection for Minority investors\n",
    "# t => Taxes\n",
    "# en => Enforcing Contracts \n",
    "# ri => Resolving Insolvencies \n",
    "d_s = pd.Series((dtf['s_procs'] + dtf['s_time'] + dtf['s_cost'] + dtf['s_min_cap']) / 4)\n",
    "d_cn = pd.Series((dtf['cn_procs'] + dtf['cn_time'] +  dtf['cn_cost']) / 3)\n",
    "d_e = pd.Series((dtf['e_procs'] + dtf['e_time'] + dtf['e_cost']) / 3)\n",
    "d_rp = pd.Series((dtf['rp_procs'] + dtf['rp_time'] + dtf['rp_cost']) / 3)\n",
    "d_ct = pd.Series((dtf['ct_s'] + dtf['ct_d']) / 2) \n",
    "d_pm = pd.Series((dtf['pm_cft'] + dtf['pm_gv']) / 2)\n",
    "d_t = pd.Series((dtf['t_p'] + dtf['t_t'] + dtf['t_tr']) / 3)\n",
    "d_en = pd.Series((dtf['en_time'] + dtf['en_cost']) / 2)\n",
    "d_ri = pd.Series((dtf['ri_r'] + dtf['ri_s']) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall average across indicators \n",
    "# I have 9 here because none of the indicators of trade costs are available for all 5 years \n",
    "# It should be easy to tweak the code to include some of the trade indicators but \n",
    "#     at the cost of restricting the analysis to the 4 data years 2014 to 2017 \n",
    "\n",
    "d_DTF = pd.Series((d_s + d_cn + d_e + d_rp + d_ct + d_pm + d_t + d_en + d_ri) / 9)\n",
    "#d_DTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df2, d_s.rename('s'), \n",
    "                 d_cn.rename('cn'), \n",
    "                 d_e.rename('e'), \n",
    "                 d_rp.rename('rp'), \n",
    "                 d_ct.rename('ct'), \n",
    "                 d_pm.rename('pm'), \n",
    "                 d_t.rename('t'), \n",
    "                 d_en.rename('en'), \n",
    "                 d_ri.rename('ri'), \n",
    "                 d_DTF.rename('DTF')], axis=1)\n",
    "#df2\n",
    "#len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(axis = 0, subset = ['DTF'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2\n",
    "#len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5 series, one for each year, indexed by country code\n",
    "#\n",
    "d_2018 = pd.Series(df2.loc[2018]['DTF'])\n",
    "#len(d_2018)\n",
    "d_2017 = pd.Series(df2.loc[2017]['DTF'])\n",
    "#len(d_2017)\n",
    "d_2016 = pd.Series(df2.loc[2016]['DTF'])\n",
    "#len(d_2016)\n",
    "d_2015 = pd.Series(df2.loc[2015]['DTF'])\n",
    "#len(d_2015)\n",
    "d_2014 = pd.Series(df2.loc[2014]['DTF'])\n",
    "#len(d_2014)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create series objects that I can sort, one for each year\n",
    "\n",
    "df_2018 = pd.DataFrame(d_2018)\n",
    "dfs_2018 = df_2018.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2018)\n",
    "dfs_2018['Rank18']= pd.Series(range(1, length + 1 ,1), index=dfs_2018.index)\n",
    "\n",
    "df_2017 = pd.DataFrame(d_2017)\n",
    "dfs_2017 = df_2017.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2017)\n",
    "dfs_2017['Rank17']= pd.Series(range(1, length + 1 ,1), index=dfs_2017.index)\n",
    "\n",
    "df_2016 = pd.DataFrame(d_2016)\n",
    "dfs_2016 = df_2016.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2016)\n",
    "dfs_2016['Rank16']= pd.Series(range(1, length + 1 ,1), index=dfs_2016.index)\n",
    "\n",
    "df_2015 = pd.DataFrame(d_2015)\n",
    "dfs_2015 = df_2015.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2015)\n",
    "dfs_2015['Rank15']= pd.Series(range(1, length + 1 ,1), index=dfs_2015.index)\n",
    "\n",
    "df_2014 = pd.DataFrame(d_2014)\n",
    "dfs_2014 = df_2014.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2014)\n",
    "dfs_2014['Rank14']= pd.Series(range(1, length + 1 ,1), index=dfs_2014.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe object dfs indexed by country code that holds sorted series  \n",
    "#\n",
    "# Combine the sorted series into a single dataframe indexed by country \n",
    "#\n",
    "dfs_all = pd.concat([dfs_2014, dfs_2015, dfs_2016, dfs_2017,dfs_2018], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the data for the row of dfs_all that has the results for Chile\n",
    "#\n",
    "dfs_all.loc['CHL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Romer's QST rank against the official DBI rank\n",
    "\n",
    "The official _World Bank_ figures can be dowloaded from [this website](https://www.doingbusiness.org/en/custom-query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = pd.read_excel('Historical-data---complete-data-with-scores.xlsx',header=1,usecols=[0,1,4,6,7,8,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Years are filtered out as to focus on the time range 2014-2018 around which the debate over Chile's rank spread. The scores for individual cities - implemented from the _Doing Business_ report 2015 onwards for countries having more than 100 million habitants - are\n",
    "excluded in order not to falsify the country's ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old=df_old[(df_old['DB Year']>2013)&(df_old['DB Year']<2019)&\n",
    "              (~df_old['Country code'].isin(['BGD_Chit','BGD','BRA_Rio','BRA','CHN_Beij','CHN',\n",
    "                                                                 'IND_Delh','IND','IDN','IDN_Sura','JPN_Osak','JPN',\n",
    "                                                                 'MEX','MEX_Mont','NGA_Kano','NGA','PAK_Laho','PAK',\n",
    "                                                                 'RUS','RUS_Sai','USA_Losa','USA']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country ranks are eventually obtained from their yearly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old.loc[df_old['DB Year']==2014,'rank']=df_old['Ease of doing business score  (DB10-14 methodology)'].rank(ascending=False)\n",
    "df_old.loc[df_old['DB Year']==2015,'rank']=df_old[df_old['DB Year']==2015]['Ease of doing business score (DB15 methodology)'].rank(ascending=False)\n",
    "df_old.loc[df_old['DB Year']==2016,'rank']=df_old[df_old['DB Year']==2016]['Ease of doing business score (DB16 methodology)'].rank(ascending=False)\n",
    "df_old.loc[df_old['DB Year']==2017,'rank']=df_old[df_old['DB Year']==2017]['Ease of doing business score (DB17-19 methodology)'].rank(ascending=False)\n",
    "df_old.loc[df_old['DB Year']==2018,'rank']=df_old[df_old['DB Year']==2018]['Ease of doing business score (DB17-19 methodology)'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two blocks of codes are aimed to harmonise the datasets as to ease their plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_all['Country code']=dfs_all.index\n",
    "dfs_all = dfs_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Romer = pd.melt(dfs_all, id_vars=['Country code'], value_vars=['Rank14', 'Rank15','Rank16','Rank17','Rank18'],\n",
    "                  var_name='DB Year', value_name='rank')\n",
    "df_Romer['DB Year']=df_Romer['DB Year'].str.replace('Rank','20').astype(int)\n",
    "df_Romer = df_Romer.sort_values(by=['Country code','DB Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A color-year mapping is then defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = dict(zip(range(2014,2019),colors_list[:5]))\n",
    "df_Romer['color']=df_Romer['DB Year'].map(color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chile's ranking is eventually extracted from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CHL = df_Romer[df_Romer['Country code']=='CHL']\n",
    "\n",
    "map_rank = dict(zip(df_old[df_old['Country code']=='CHL']['DB Year'],df_old[df_old['Country code']=='CHL']['rank']))\n",
    "\n",
    "df_CHL['rank DBI']=df_CHL['DB Year'].map(map_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the official DBI figures are then compared against Romer's QST are in Figure 4 produced as per the code below. Including only the component indicators consistently available over the time frame inquired produced a less volatile ranks for Chile as Paul Romer remarked in [his blog](https://paulromer.net/doing-business/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi= 80)\n",
    "for index, row in df_CHL.iterrows():\n",
    "    plt.scatter('Romer QST',row['rank'], color = row['color'], s=10, label = row['DB Year'])\n",
    "    plt.scatter('Official DBI',row['rank DBI'], color = row['color'], s=10,label='')\n",
    "plt.ylabel('Rank of Chile')\n",
    "plt.xlabel('Accounting method')\n",
    "plt.ylim(34,54)\n",
    "plt.xticks(fontsize=10)\n"
    "plt.yticks(np.arange(30, 55, step=5))\n",
    "plt.legend(loc='center')\n",
    "plt.gca().spines[\"top\"].set_alpha(0.0)    \n",
    "plt.gca().spines[\"bottom\"].set_alpha(0.3)\n",
    "plt.gca().spines[\"right\"].set_alpha(0.0)    \n",
    "plt.gca().spines[\"left\"].set_alpha(0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
